{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training Demo for llama on SageMaker\n",
    "\n",
    "### Model Parallelism using SageMaker model parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Development environment](#Development-environment)  \n",
    "    3. [Permissions](#Permissions)\n",
    "3. [Processing](#Preprocessing)   \n",
    "    1. [Tokenization](#Tokenization)  \n",
    "    2. [Uploading data to sagemaker_session_bucket](#Uploading-data-to-sagemaker_session_bucket)  \n",
    "4. [Fine-tuning & starting Sagemaker Training Job](#Fine-tuning-\\&-starting-Sagemaker-Training-Job)  \n",
    "    1. [Creating an Estimator and start a training job](#Creating-an-Estimator-and-start-a-training-job)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to our end-to-end distributed training example. In this demo, we will use the Hugging Face `transformers` and `datasets` library together with a Amazon sagemaker-sdk extension on a multi-node multi-gpu cluster using [SageMaker Model Parallelism Library](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-intro.html). The demo will use the new smdistributed library to run training on multiple gpus. \n",
    "\n",
    "_**NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "_*Note:* we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you haven´t it installed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.156.0 in /opt/conda/lib/python3.7/site-packages (2.156.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (23.1)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (3.2.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (1.0.1)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (3.20.3)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (0.7.5)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (2.2.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (4.13.0)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (1.21.6)\n",
      "Requirement already satisfied: PyYAML==6.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (6.0)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (0.1.5)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (0.3.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (3.2.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (23.1.0)\n",
      "Requirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (1.7.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (1.26.135)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (1.3.5)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.135 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.156.0) (1.29.135)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.156.0) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.156.0) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker>=2.156.0) (4.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker>=2.156.0) (3.15.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker>=2.156.0) (1.14.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->sagemaker>=2.156.0) (0.15.7)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from jsonschema->sagemaker>=2.156.0) (59.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>=2.156.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>=2.156.0) (2019.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.156.0) (1.7.6.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.156.0) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.156.0) (0.70.14)\n",
      "Requirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.156.0) (0.3.6)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.7/site-packages (from schema->sagemaker>=2.156.0) (0.6.0.post1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.135->boto3<2.0,>=1.26.131->sagemaker>=2.156.0) (1.26.15)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.156.0\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After upgrading the sagemaker sdk library, please restart the jupyter kernel and execute the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.156.0\n"
     ]
    }
   ],
   "source": [
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::514385905925:role/service-role/AmazonSageMaker-ExecutionRole-20201218T184365\n",
      "sagemaker bucket: sagemaker-us-east-1-514385905925\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create a sagemaker training job we need an Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in .....\n",
    "\n",
    "When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the training container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container at `/opt/ml/input/data`. Then, it starts the training job by running. \n",
    "\n",
    "The `hyperparameters` you define in the estimator are passed in as named arguments. \n",
    "\n",
    "Sagemaker is providing useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "* `SM_CHANNEL_XXXX:` A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimator’s fit call, named `train` and `test`, the environment variables `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` are set.\n",
    "\n",
    "\n",
    "To run your training job locally you can define `instance_type='local'` or `instance_type='local_gpu'` for gpu usage. _Note: this does not working within SageMaker Studio_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "#hyperparameters for llama\n",
    "hyperparameters={\n",
    "  'training_dir': '/opt/ml/input/data/train', # path where sagemaker will save training dataset\n",
    "  'test_dir': '/opt/ml/input/data/test',      # path where sagemaker will save test dataset\n",
    "  'num_train_epochs': 1,                                         # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                    # batch size for training\n",
    "  'per_device_eval_batch_size': 2,                     # batch size for evaluation\n",
    "  'learning_rate': 1e-5,   \n",
    "  'gradient_accumulation_steps': 4,\n",
    "  'model_max_length': 1536                          # learning rate used during training\n",
    "}\n",
    "\n",
    "# configuration for running training on smdistributed Model Parallel\n",
    "mpi_options = {\n",
    "    \"enabled\" : True,\n",
    "    \"processes_per_host\" : 8,\n",
    "}\n",
    "\n",
    "smp_options = {\n",
    "    \"enabled\":True,\n",
    "    \"parameters\": {\n",
    "        \"pipeline_parallel_degree\": 16,\n",
    "        \"placement_strategy\": \"cluster\",\n",
    "        \"tensor_parallel_degree\": 1,\n",
    "        \"partitions\": 16,\n",
    "        \"fp16\": True,\n",
    "        \"ddp\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "distribution={\n",
    "    \"smdistributed\": {\"modelparallel\": smp_options},\n",
    "    \"mpi\": mpi_options\n",
    "}\n",
    "\n",
    "# instance configurations\n",
    "instance_type='ml.p4d.24xlarge'\n",
    "instance_count = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimator\n",
    "#define the model s3 path which will store your trained model asset\n",
    "#Note: you should use your real s3 path to configure model_s3_path\n",
    "target_model_s3_path='s3://your_bucket/llama-smp-finetuned-052111/model/'\n",
    "\n",
    "#define the s3 path of source model before training.  \n",
    "#Note: Please add the wildcard character '*' in the following path, otherwise error will happen.\n",
    "source_model_s3_path = 's3://your_bucket/llama/pretrained/7B/model/*'\n",
    "\n",
    "environment = {'CUDA_LAUNCH_BLOCKING': '1',\n",
    "               'SOURCE_MODEL_BEFORE_TRAINING_S3_PATH': source_model_s3_path,\n",
    "               'TARGET_MODEL_AFTER_TRAINING_S3_PATH': target_model_s3_path}\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "'''\n",
    "huggingface_estimator = HuggingFace(entry_point='train-llama-file-lock-for-HF-container.py',\n",
    "                                    source_dir           = '.', \n",
    "                                    instance_type=instance_type,\n",
    "                                    instance_count=instance_count,\n",
    "                                    role=role,\n",
    "                                    transformers_version='4.17',\n",
    "                                    pytorch_version='1.10',\n",
    "                                    py_version='py38',\n",
    "                                    distribution= distribution,\n",
    "                                    hyperparameters = hyperparameters,\n",
    "                                    environment = environment,\n",
    "                                    debugger_hook_config=False)\n",
    "'''\n",
    "\n",
    "huggingface_estimator = PyTorch(entry_point='train-llama-for-pytorch-container.py',\n",
    "                                source_dir           = '.', \n",
    "                                instance_type=instance_type,\n",
    "                                instance_count=instance_count,\n",
    "                                role=role,\n",
    "                                framework_version='1.12.0',\n",
    "                                py_version='py38',\n",
    "                                distribution= distribution,\n",
    "                                hyperparameters = hyperparameters,\n",
    "                                environment = environment,\n",
    "                                debugger_hook_config=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_dir': '\"/opt/ml/input/data/train\"',\n",
       " 'test_dir': '\"/opt/ml/input/data/test\"',\n",
       " 'num_train_epochs': '1',\n",
       " 'per_device_train_batch_size': '2',\n",
       " 'per_device_eval_batch_size': '2',\n",
       " 'learning_rate': '1e-05',\n",
       " 'gradient_accumulation_steps': '4',\n",
       " 'model_max_length': '1536',\n",
       " 'sagemaker_mpi_enabled': 'true',\n",
       " 'sagemaker_mpi_num_of_processes_per_host': '8',\n",
       " 'sagemaker_mpi_custom_mpi_options': '\"\"',\n",
       " 'mp_parameters': '{\"pipeline_parallel_degree\": 16, \"placement_strategy\": \"cluster\", \"tensor_parallel_degree\": 1, \"partitions\": 16, \"fp16\": true, \"ddp\": true}',\n",
       " 'sagemaker_distributed_dataparallel_enabled': 'false',\n",
       " 'sagemaker_instance_type': '\"ml.p4d.24xlarge\"'}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-05-21-03-28-47-224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-21 03:29:55 Starting - Starting the training job......\n",
      "2023-05-21 03:30:34 Starting - Preparing the instances for training.....................\n",
      "2023-05-21 03:34:23 Downloading - Downloading input data...\n",
      "2023-05-21 03:34:48 Training - Downloading the training image..................\n",
      "2023-05-21 03:37:29 Training - Training image download completed. Training in progress.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:25,041 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:25,111 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:25,113 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:25,445 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:25,516 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:25,518 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:29,345 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers@ git+https://github.com/huggingface/transformers.git@97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/transformers.git (to revision 97a3d16a6941294d7d76d24f36f26617d224278e) to /tmp/pip-install-99026zqk/transformers_429a703eaa7a427bae1d734b18480741\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-99026zqk/transformers_429a703eaa7a427bae1d734b18480741\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:29,530 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mCollecting transformers@ git+https://github.com/huggingface/transformers.git@97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[35mCloning https://github.com/huggingface/transformers.git (to revision 97a3d16a6941294d7d76d24f36f26617d224278e) to /tmp/pip-install-ijim08u_/transformers_75a17121a0ab41df9a4cf4f08230c715\u001b[0m\n",
      "\u001b[35mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-ijim08u_/transformers_75a17121a0ab41df9a4cf4f08230c715\u001b[0m\n",
      "\u001b[34mRunning command git rev-parse -q --verify 'sha^97a3d16a6941294d7d76d24f36f26617d224278e'\u001b[0m\n",
      "\u001b[34mRunning command git fetch -q https://github.com/huggingface/transformers.git 97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[35mRunning command git rev-parse -q --verify 'sha^97a3d16a6941294d7d76d24f36f26617d224278e'\u001b[0m\n",
      "\u001b[35mRunning command git fetch -q https://github.com/huggingface/transformers.git 97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[34mRunning command git checkout -q 97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[35mRunning command git checkout -q 97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/transformers.git to commit 97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[35mResolved https://github.com/huggingface/transformers.git to commit 97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[35mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[35mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[35mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.10.1\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.10.1-py3-none-any.whl (469 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 469.0/469.0 kB 36.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting evaluate==0.4.0\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 23.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.10.2.3)\u001b[0m\n",
      "\u001b[34mCollecting rouge-score\u001b[0m\n",
      "\u001b[34mDownloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 69.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (1.22.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (4.64.0)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (2.28.1)\u001b[0m\n",
      "\u001b[34mCollecting xxhash\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 213.0/213.0 kB 42.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (0.70.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (9.0.0)\u001b[0m\n",
      "\u001b[35mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting datasets==2.10.1\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.10.1-py3-none-any.whl (469 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 469.0/469.0 kB 36.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting evaluate==0.4.0\u001b[0m\n",
      "\u001b[35mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 22.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ninja in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.10.2.3)\u001b[0m\n",
      "\u001b[35mCollecting rouge-score\u001b[0m\n",
      "\u001b[35mDownloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting sentencepiece\u001b[0m\n",
      "\u001b[35mDownloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 85.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (2022.7.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (0.70.13)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (0.3.5.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (1.4.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (5.4.1)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub<1.0.0,>=0.2.0\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 48.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (4.64.0)\u001b[0m\n",
      "\u001b[35mCollecting responses<0.19\u001b[0m\n",
      "\u001b[35mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (9.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (2.28.1)\u001b[0m\n",
      "\u001b[35mCollecting xxhash\u001b[0m\n",
      "\u001b[35mDownloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 213.0/213.0 kB 43.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiohttp\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 77.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (0.3.5.1)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0.0,>=0.2.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 39.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (1.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (2022.7.1)\u001b[0m\n",
      "\u001b[35mCollecting aiohttp\u001b[0m\n",
      "\u001b[35mDownloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 89.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (1.22.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (21.3)\u001b[0m\n",
      "\u001b[35mCollecting filelock\u001b[0m\n",
      "\u001b[35mDownloading filelock-3.12.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[35mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 105.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting regex!=2019.12.17\u001b[0m\n",
      "\u001b[35mDownloading regex-2023.5.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 771.9/771.9 kB 74.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting absl-py\u001b[0m\n",
      "\u001b[35mDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 33.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting nltk\u001b[0m\n",
      "\u001b[35mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 97.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.8/site-packages (from rouge-score->-r requirements.txt (line 5)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1->-r requirements.txt (line 2)) (4.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets==2.10.1->-r requirements.txt (line 2)) (3.0.9)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (2022.6.15)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (1.26.11)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (2.1.1)\u001b[0m\n",
      "\u001b[35mCollecting multidict<7.0,>=4.5\u001b[0m\n",
      "\u001b[35mDownloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 26.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (21.4.0)\u001b[0m\n",
      "\u001b[35mCollecting frozenlist>=1.1.1\u001b[0m\n",
      "\u001b[35mDownloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.3/161.3 kB 40.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting aiosignal>=1.1.2\u001b[0m\n",
      "\u001b[35mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[35mCollecting yarl<2.0,>=1.0\u001b[0m\n",
      "\u001b[35mDownloading yarl-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (266 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 266.9/266.9 kB 55.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting async-timeout<5.0,>=4.0.0a3\u001b[0m\n",
      "\u001b[35mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk->rouge-score->-r requirements.txt (line 5)) (8.1.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk->rouge-score->-r requirements.txt (line 5)) (1.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 2)) (2022.2.1)\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: transformers, rouge-score\u001b[0m\n",
      "\u001b[35mBuilding wheel for transformers (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for transformers (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for transformers: filename=transformers-4.28.0.dev0-py3-none-any.whl size=6756543 sha256=38c9e09fac67debf31b0ce55bfed6de023b856a31c2ff86a3622da36daec23d9\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/5c/1d/64/c9c142433b568c604e359bd903fe1424c3707274fecc075ace\u001b[0m\n",
      "\u001b[35mBuilding wheel for rouge-score (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for rouge-score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=05bc249bb283370bf4c6580ccc8787ed013649d7f4610eafd3eaef7d42564884\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/24/55/6f/ebfc4cb176d1c9665da4e306e1705496206d08215c1acd9dde\u001b[0m\n",
      "\u001b[35mSuccessfully built transformers rouge-score\u001b[0m\n",
      "\u001b[35mInstalling collected packages: tokenizers, sentencepiece, xxhash, regex, multidict, frozenlist, filelock, async-timeout, absl-py, yarl, responses, nltk, huggingface-hub, aiosignal, transformers, rouge-score, aiohttp, datasets, evaluate\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.5.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 771.9/771.9 kB 70.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting filelock\u001b[0m\n",
      "\u001b[34mDownloading filelock-3.12.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 108.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting absl-py\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 28.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 96.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.8/site-packages (from rouge-score->-r requirements.txt (line 5)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1->-r requirements.txt (line 2)) (4.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets==2.10.1->-r requirements.txt (line 2)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (2022.6.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (1.26.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (3.3)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (21.4.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 24.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.3/161.3 kB 38.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (266 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 266.9/266.9 kB 48.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk->rouge-score->-r requirements.txt (line 5)) (8.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk->rouge-score->-r requirements.txt (line 5)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 2)) (2022.2.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: transformers, rouge-score\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for transformers: filename=transformers-4.28.0.dev0-py3-none-any.whl size=6756543 sha256=0b318410909e1291765d89f768165ce3d8614d80cd167d6ea26a90f385f783ba\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/5c/1d/64/c9c142433b568c604e359bd903fe1424c3707274fecc075ace\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge-score (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge-score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=05bc249bb283370bf4c6580ccc8787ed013649d7f4610eafd3eaef7d42564884\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/24/55/6f/ebfc4cb176d1c9665da4e306e1705496206d08215c1acd9dde\u001b[0m\n",
      "\u001b[34mSuccessfully built transformers rouge-score\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, sentencepiece, xxhash, regex, multidict, frozenlist, filelock, async-timeout, absl-py, yarl, responses, nltk, huggingface-hub, aiosignal, transformers, rouge-score, aiohttp, datasets, evaluate\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.4.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.10.1 evaluate-0.4.0 filelock-3.12.0 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 nltk-3.8.1 regex-2023.5.5 responses-0.18.0 rouge-score-0.1.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.28.0.dev0 xxhash-3.2.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 23.1.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:53,166 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:53,166 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:53,307 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:53,307 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:53,311 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:53,318 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:53,318 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.209.56\u001b[0m\n",
      "\u001b[35mSuccessfully installed absl-py-1.4.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.10.1 evaluate-0.4.0 filelock-3.12.0 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 nltk-3.8.1 regex-2023.5.5 responses-0.18.0 rouge-score-0.1.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.28.0.dev0 xxhash-3.2.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip available: 22.2.2 -> 23.1.2\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:53,477 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:53,477 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:53,623 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:53,624 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:53,634 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:53,695 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:53,696 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:53,696 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:53,696 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:53,702 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:54,328 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:54,392 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:54,392 sagemaker-training-toolkit INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:54,392 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:54,392 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1', 'algo-2'] Hosts: ['algo-1:8', 'algo-2:8'] process_per_hosts: 8 num_processes: 16\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:54,394 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:54,468 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 8\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"gradient_accumulation_steps\": 4,\n",
      "        \"learning_rate\": 1e-05,\n",
      "        \"model_max_length\": 1536,\n",
      "        \"mp_parameters\": {\n",
      "            \"pipeline_parallel_degree\": 16,\n",
      "            \"placement_strategy\": \"cluster\",\n",
      "            \"tensor_parallel_degree\": 1,\n",
      "            \"partitions\": 16,\n",
      "            \"fp16\": true,\n",
      "            \"ddp\": true\n",
      "        },\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"per_device_eval_batch_size\": 2,\n",
      "        \"per_device_train_batch_size\": 2,\n",
      "        \"test_dir\": \"/opt/ml/input/data/test\",\n",
      "        \"training_dir\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-05-21-03-28-47-224\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-514385905925/pytorch-training-2023-05-21-03-28-47-224/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train-llama-for-pytorch-container\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train-llama-for-pytorch-container.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"gradient_accumulation_steps\":4,\"learning_rate\":1e-05,\"model_max_length\":1536,\"mp_parameters\":{\"ddp\":true,\"fp16\":true,\"partitions\":16,\"pipeline_parallel_degree\":16,\"placement_strategy\":\"cluster\",\"tensor_parallel_degree\":1},\"num_train_epochs\":1,\"per_device_eval_batch_size\":2,\"per_device_train_batch_size\":2,\"test_dir\":\"/opt/ml/input/data/test\",\"training_dir\":\"/opt/ml/input/data/train\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train-llama-for-pytorch-container.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train-llama-for-pytorch-container\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-514385905925/pytorch-training-2023-05-21-03-28-47-224/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"gradient_accumulation_steps\":4,\"learning_rate\":1e-05,\"model_max_length\":1536,\"mp_parameters\":{\"ddp\":true,\"fp16\":true,\"partitions\":16,\"pipeline_parallel_degree\":16,\"placement_strategy\":\"cluster\",\"tensor_parallel_degree\":1},\"num_train_epochs\":1,\"per_device_eval_batch_size\":2,\"per_device_train_batch_size\":2,\"test_dir\":\"/opt/ml/input/data/test\",\"training_dir\":\"/opt/ml/input/data/train\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"pytorch-training-2023-05-21-03-28-47-224\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-514385905925/pytorch-training-2023-05-21-03-28-47-224/source/sourcedir.tar.gz\",\"module_name\":\"train-llama-for-pytorch-container\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train-llama-for-pytorch-container.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--gradient_accumulation_steps\",\"4\",\"--learning_rate\",\"1e-05\",\"--model_max_length\",\"1536\",\"--mp_parameters\",\"ddp=True,fp16=True,partitions=16,pipeline_parallel_degree=16,placement_strategy=cluster,tensor_parallel_degree=1\",\"--num_train_epochs\",\"1\",\"--per_device_eval_batch_size\",\"2\",\"--per_device_train_batch_size\",\"2\",\"--test_dir\",\"/opt/ml/input/data/test\",\"--training_dir\",\"/opt/ml/input/data/train\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=4\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=1e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_MAX_LENGTH=1536\u001b[0m\n",
      "\u001b[34mSM_HP_MP_PARAMETERS={\"ddp\":true,\"fp16\":true,\"partitions\":16,\"pipeline_parallel_degree\":16,\"placement_strategy\":\"cluster\",\"tensor_parallel_degree\":1}\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_TEST_DIR=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_HP_TRAINING_DIR=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_CURRENT_INSTANCE_TYPE -x SM_CURRENT_INSTANCE_GROUP -x SM_CURRENT_INSTANCE_GROUP_HOSTS -x SM_INSTANCE_GROUPS -x SM_INSTANCE_GROUPS_DICT -x SM_DISTRIBUTION_INSTANCE_GROUPS -x SM_IS_HETERO -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TEST -x SM_CHANNEL_TRAIN -x SM_HP_GRADIENT_ACCUMULATION_STEPS -x SM_HP_LEARNING_RATE -x SM_HP_MODEL_MAX_LENGTH -x SM_HP_MP_PARAMETERS -x SM_HP_NUM_TRAIN_EPOCHS -x SM_HP_PER_DEVICE_EVAL_BATCH_SIZE -x SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE -x SM_HP_TEST_DIR -x SM_HP_TRAINING_DIR -x PYTHONPATH /opt/conda/bin/python3.8 -m mpi4py train-llama-for-pytorch-container.py --gradient_accumulation_steps 4 --learning_rate 1e-05 --model_max_length 1536 --mp_parameters ddp=True,fp16=True,partitions=16,pipeline_parallel_degree=16,placement_strategy=cluster,tensor_parallel_degree=1 --num_train_epochs 1 --per_device_eval_batch_size 2 --per_device_train_batch_size 2 --test_dir /opt/ml/input/data/test --training_dir /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2023-05-21 03:38:55.030: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.0.209.56' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[34mData for JOB [41012,1] offset 0 Total slots allocated 16\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: algo-1#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 7 Bound: N/A\n",
      " Data for node: algo-2#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 8 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 9 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 10 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 11 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 12 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 13 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 14 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 15 Bound: N/A\n",
      " =============================================================\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:55,707 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=218, name='orted', status='sleeping', started='03:38:55')]\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:55,708 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=218, name='orted', status='sleeping', started='03:38:55')]\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:55,708 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=218, name='orted', status='sleeping', started='03:38:55')]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 03:38:56.856: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 03:38:56.889: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 03:38:56.920: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 03:38:56.928: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 03:38:56.929: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:56.942: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 03:38:56.942: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 03:38:56.942: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 03:38:56.942: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 03:38:56.945: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 03:38:56.952: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 03:38:56.957: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 03:38:56.957: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 03:38:56.957: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 03:38:56.960: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 03:38:56.964: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 03:38:58.759: I smdistributed/modelparallel/torch/state_mod.py:162] [6] Finished initializing torch distributed process groups. pp_rank: 6, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 03:38:58.759: I smdistributed/modelparallel/torch/state_mod.py:162] [1] Finished initializing torch distributed process groups. pp_rank: 1, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 03:38:58.759: I smdistributed/modelparallel/torch/state_mod.py:162] [9] Finished initializing torch distributed process groups. pp_rank: 9, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 03:38:58.760: I smdistributed/modelparallel/torch/state_mod.py:162] [11] Finished initializing torch distributed process groups. pp_rank: 11, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 03:38:58.761: I smdistributed/modelparallel/torch/state_mod.py:162] [15] Finished initializing torch distributed process groups. pp_rank: 15, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 03:38:58.763: I smdistributed/modelparallel/torch/state_mod.py:162] [7] Finished initializing torch distributed process groups. pp_rank: 7, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 03:38:58.763: I smdistributed/modelparallel/torch/state_mod.py:162] [2] Finished initializing torch distributed process groups. pp_rank: 2, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 03:38:58.764: I smdistributed/modelparallel/torch/state_mod.py:162] [10] Finished initializing torch distributed process groups. pp_rank: 10, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 03:38:58.764: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 03:38:58.764: I smdistributed/modelparallel/torch/state_mod.py:162] [8] Finished initializing torch distributed process groups. pp_rank: 8, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 03:38:58.765: I smdistributed/modelparallel/torch/state_mod.py:162] [13] Finished initializing torch distributed process groups. pp_rank: 13, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 03:38:58.766: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 03:38:58.766: I smdistributed/modelparallel/torch/state_mod.py:162] [12] Finished initializing torch distributed process groups. pp_rank: 12, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 03:38:58.766: I smdistributed/modelparallel/torch/state_mod.py:162] [14] Finished initializing torch distributed process groups. pp_rank: 14, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 03:38:58.767: I smdistributed/modelparallel/torch/state_mod.py:162] [3] Finished initializing torch distributed process groups. pp_rank: 3, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:58.768: I smdistributed/modelparallel/torch/state_mod.py:162] [0] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:58.768: I smdistributed/modelparallel/torch/throttler.py:37] Using NCCL throttle limit of 8.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 03:38:58.769: I smdistributed/modelparallel/torch/state_mod.py:162] [5] Finished initializing torch distributed process groups. pp_rank: 5, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 03:38:58.769: I smdistributed/modelparallel/torch/state_mod.py:162] [4] Finished initializing torch distributed process groups. pp_rank: 4, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 03:38:58.772: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 03:38:58.772: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 03:38:58.772: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 03:38:58.775: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 03:38:58.777: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 03:38:58.778: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 03:38:58.778: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 03:38:58.778: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 03:38:58.780: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 03:38:58.781: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 03:38:58.782: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 03:38:58.781: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.604: I smdistributed/modelparallel/backend/config.py:293] Configuration parameters:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.604: I smdistributed/modelparallel/backend/config.py:296]   pipeline_parallel_degree: 16\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.604: I smdistributed/modelparallel/backend/config.py:296]   microbatches: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.604: I smdistributed/modelparallel/backend/config.py:296]   pipeline: interleaved\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   horovod: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   ddp: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   tensor_parallel_degree: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   sdp_reduce_bucket_size: 500000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   sdp_param_persistence_threshold: 1000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   sdp_max_live_parameters: 1000000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   sdp_hierarchical_allgather: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   sdp_gradient_clipping: 1.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   ddp_port: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   ddp_dist_backend: nccl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   contiguous: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   placement_strategy: cluster\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   optimize: speed\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   default_partition: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   auto_partition: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   prescaled_batch: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   memory_weight: 0.8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   active_microbatches: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   fp16: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   bf16: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   fp16_params: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   tensor_parallel_seed: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   offload_activations: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   shard_optimizer_state: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   sharded_data_parallel_degree: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   delayed_parameter_initialization: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   skip_tracing: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   activation_loading_horizon: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: W smdistributed/modelparallel/backend/config.py:301] WARNING: \"fp16_params\" is a deprecated config key, please use \"fp16\" instead\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.607: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 03:38:59.787: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:-----------local rank 0 downloading model from s3----\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/.gitattributes /tmp/llama_source/.gitattributes\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/generation_config.json /tmp/llama_source/generation_config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/config.json /tmp/llama_source/config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/README.md /tmp/llama_source/README.md\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/special_tokens_map.json /tmp/llama_source/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/LICENSE /tmp/llama_source/LICENSE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/tokenizer_config.json /tmp/llama_source/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model.bin.index.json /tmp/llama_source/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/tokenizer.model /tmp/llama_source/tokenizer.model\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:-----------local rank 0 downloading model from s3----\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/generation_config.json /tmp/llama_source/generation_config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/special_tokens_map.json /tmp/llama_source/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/config.json /tmp/llama_source/config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/README.md /tmp/llama_source/README.md\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/.gitattributes /tmp/llama_source/.gitattributes\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/tokenizer_config.json /tmp/llama_source/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model.bin.index.json /tmp/llama_source/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/LICENSE /tmp/llama_source/LICENSE\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/tokenizer.model /tmp/llama_source/tokenizer.model\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00028-of-00033.bin /tmp/llama_source/pytorch_model-00028-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00028-of-00033.bin /tmp/llama_source/pytorch_model-00028-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00024-of-00033.bin /tmp/llama_source/pytorch_model-00024-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00031-of-00033.bin /tmp/llama_source/pytorch_model-00031-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00024-of-00033.bin /tmp/llama_source/pytorch_model-00024-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00003-of-00033.bin /tmp/llama_source/pytorch_model-00003-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00003-of-00033.bin /tmp/llama_source/pytorch_model-00003-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00015-of-00033.bin /tmp/llama_source/pytorch_model-00015-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00025-of-00033.bin /tmp/llama_source/pytorch_model-00025-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00019-of-00033.bin /tmp/llama_source/pytorch_model-00019-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00014-of-00033.bin /tmp/llama_source/pytorch_model-00014-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00014-of-00033.bin /tmp/llama_source/pytorch_model-00014-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00031-of-00033.bin /tmp/llama_source/pytorch_model-00031-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00019-of-00033.bin /tmp/llama_source/pytorch_model-00019-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00025-of-00033.bin /tmp/llama_source/pytorch_model-00025-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00015-of-00033.bin /tmp/llama_source/pytorch_model-00015-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00001-of-00033.bin /tmp/llama_source/pytorch_model-00001-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00009-of-00033.bin /tmp/llama_source/pytorch_model-00009-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00001-of-00033.bin /tmp/llama_source/pytorch_model-00001-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00006-of-00033.bin /tmp/llama_source/pytorch_model-00006-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00029-of-00033.bin /tmp/llama_source/pytorch_model-00029-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00010-of-00033.bin /tmp/llama_source/pytorch_model-00010-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00005-of-00033.bin /tmp/llama_source/pytorch_model-00005-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00010-of-00033.bin /tmp/llama_source/pytorch_model-00010-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00005-of-00033.bin /tmp/llama_source/pytorch_model-00005-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00013-of-00033.bin /tmp/llama_source/pytorch_model-00013-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00009-of-00033.bin /tmp/llama_source/pytorch_model-00009-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00029-of-00033.bin /tmp/llama_source/pytorch_model-00029-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00018-of-00033.bin /tmp/llama_source/pytorch_model-00018-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00026-of-00033.bin /tmp/llama_source/pytorch_model-00026-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00026-of-00033.bin /tmp/llama_source/pytorch_model-00026-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00023-of-00033.bin /tmp/llama_source/pytorch_model-00023-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00018-of-00033.bin /tmp/llama_source/pytorch_model-00018-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00013-of-00033.bin /tmp/llama_source/pytorch_model-00013-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00002-of-00033.bin /tmp/llama_source/pytorch_model-00002-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00022-of-00033.bin /tmp/llama_source/pytorch_model-00022-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00023-of-00033.bin /tmp/llama_source/pytorch_model-00023-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00021-of-00033.bin /tmp/llama_source/pytorch_model-00021-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00006-of-00033.bin /tmp/llama_source/pytorch_model-00006-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00022-of-00033.bin /tmp/llama_source/pytorch_model-00022-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00021-of-00033.bin /tmp/llama_source/pytorch_model-00021-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00002-of-00033.bin /tmp/llama_source/pytorch_model-00002-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00012-of-00033.bin /tmp/llama_source/pytorch_model-00012-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00004-of-00033.bin /tmp/llama_source/pytorch_model-00004-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00004-of-00033.bin /tmp/llama_source/pytorch_model-00004-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00032-of-00033.bin /tmp/llama_source/pytorch_model-00032-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00020-of-00033.bin /tmp/llama_source/pytorch_model-00020-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00032-of-00033.bin /tmp/llama_source/pytorch_model-00032-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00012-of-00033.bin /tmp/llama_source/pytorch_model-00012-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00020-of-00033.bin /tmp/llama_source/pytorch_model-00020-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00017-of-00033.bin /tmp/llama_source/pytorch_model-00017-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00033-of-00033.bin /tmp/llama_source/pytorch_model-00033-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00017-of-00033.bin /tmp/llama_source/pytorch_model-00017-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00008-of-00033.bin /tmp/llama_source/pytorch_model-00008-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00008-of-00033.bin /tmp/llama_source/pytorch_model-00008-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00033-of-00033.bin /tmp/llama_source/pytorch_model-00033-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00027-of-00033.bin /tmp/llama_source/pytorch_model-00027-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00027-of-00033.bin /tmp/llama_source/pytorch_model-00027-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00016-of-00033.bin /tmp/llama_source/pytorch_model-00016-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00011-of-00033.bin /tmp/llama_source/pytorch_model-00011-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00016-of-00033.bin /tmp/llama_source/pytorch_model-00016-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00011-of-00033.bin /tmp/llama_source/pytorch_model-00011-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00030-of-00033.bin /tmp/llama_source/pytorch_model-00030-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00030-of-00033.bin /tmp/llama_source/pytorch_model-00030-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00007-of-00033.bin /tmp/llama_source/pytorch_model-00007-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00007-of-00033.bin /tmp/llama_source/pytorch_model-00007-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:14,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:14,  2.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:14,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:14,  2.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:14,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:14,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:14,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:15,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:14,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:14,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:14,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:15,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:14,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:13,  2.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:14,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:14,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:14,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:16,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:14,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:15,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:13,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:14,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:14,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:14,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:14,  2.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:13,  2.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:14,  2.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:13,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:13,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:15,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:13,  2.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:13,  2.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:15,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:13,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:14,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:14,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:13,  2.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:13,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:13,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:13,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:13,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:13,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:15,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:14,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:13,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:15,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:12,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:13,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:12,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:13,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:15,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:14,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:12,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:12,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:14,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:12,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:13,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:12,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:12,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:16,  1.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:15,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:13,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:12,  2.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:15,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:14,  1.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:12,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:13,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:11,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:14,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:12,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:11,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:15,  1.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:15,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:12,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:14,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:13,  1.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:11,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:11,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:11,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:14,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:11,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:14,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:15,  1.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:11,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:14,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:11,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:14,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:14,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:11,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:10,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:13,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.04it/s][1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:10,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:14,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:13,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:13,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:13,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:10,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:09,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:11,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:09,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:13,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:09,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:09,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  1.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:12,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:09,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:08,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:10,  1.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.02it/s][1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:08,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:12,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:08,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:07<00:09,  1.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.01it/s][1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:11,  1.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:08,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:11,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:07,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:09,  1.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:07,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.07it/s][1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.01it/s][1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:07,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:11,  1.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:11,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:07,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:08<00:08,  1.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:06,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:10,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:10,  1.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:06,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:08,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:08<00:06,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:10,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:06,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:07<00:10,  1.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:08<00:06,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:08<00:06,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:08<00:06,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:06,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:06,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:06,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:07<00:09,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:09,  1.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:05,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:06,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:09<00:05,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:06,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:06,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:09<00:05,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:06,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:09<00:05,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:09<00:05,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:08<00:09,  1.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:09<00:05,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:05,  2.03it/s][1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:05,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:05,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:08<00:08,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:08,  1.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:10<00:04,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:05,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:05,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.03it/s][1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:05,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:10<00:04,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:05,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:10<00:04,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:10<00:04,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:10<00:04,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:09<00:08,  1.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:04,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:04,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:04,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:06,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:09<00:07,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:10<00:04,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:11<00:03,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:04,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:04,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:07,  1.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:10<00:04,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:11<00:03,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:06,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:04,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:06,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:11<00:03,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:11<00:03,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:11<00:03,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:11<00:03,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:03,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:03,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:03,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:10<00:07,  1.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:05,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:10<00:06,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:11<00:03,  2.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:12<00:02,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:03,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:03,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:12<00:02,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:11<00:03,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:05,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:03,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:11<00:06,  1.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:12<00:02,  2.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:12<00:02,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:05,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:12<00:02,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:12<00:02,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:02,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:02,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:02,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:04,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:12<00:02,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:11<00:06,  1.81it/s][1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:11<00:05,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:13<00:01,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:02,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:13<00:01,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:03,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:12<00:02,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:04,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:02,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.05it/s][1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:13<00:01,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:13<00:01,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:05,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:13<00:01,  2.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:13<00:01,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:12<00:05,  1.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  1.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:01,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:01,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:01,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:03,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:13<00:01,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:14<00:00,  2.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:12<00:04,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:12<00:04,  1.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:01,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:14<00:00,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:13<00:01,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:02,  1.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:01,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:03,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:14<00:00,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:14<00:00,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:14<00:00,  2.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:14<00:00,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:04,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:13<00:04,  1.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  2.12it/s][1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  1.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:02,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:14<00:00,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:13<00:03,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  1.99it/s][1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:13<00:03,  1.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  1.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:00,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:14<00:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:02,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:01,  1.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  1.96it/s]#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  1.96it/s][1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:03,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  1.95it/s][1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:14<00:03,  1.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:01,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  1.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.91it/s][1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:14<00:02,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:01,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:14<00:02,  1.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.90it/s]#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.85it/s][1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:01,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:00,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:15<00:02,  1.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:15<00:01,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:00,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:15<00:01,  1.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:00,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.95it/s][1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:16<00:01,  1.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.95it/s][1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:17<00:00,  1.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:17<00:00,  1.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:17<00:00,  1.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 03:40:20.581: I smdistributed/modelparallel/torch/model.py:137] [12] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 03:40:20.603: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [12] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 03:40:20.653 algo-2:225 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 03:40:20.779 algo-2:225 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 03:40:20.789: I smdistributed/modelparallel/torch/model.py:137] [6] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 03:40:20.810: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [6] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 03:40:20.859 algo-1:221 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 03:40:20.981 algo-1:221 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 03:40:21.284: I smdistributed/modelparallel/torch/model.py:137] [8] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 03:40:21.305: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [8] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  0%|          | 0/200 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 03:40:21.351: I smdistributed/modelparallel/torch/model.py:137] [3] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 03:40:21.354 algo-2:221 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 03:40:21.374: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [3] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 03:40:21.429 algo-1:218 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 03:40:21.479 algo-2:221 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 03:40:21.564 algo-1:218 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 03:40:21.575: I smdistributed/modelparallel/torch/model.py:137] [2] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 03:40:21.597: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [2] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 03:40:21.649 algo-1:217 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 03:40:21.779 algo-1:217 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 03:40:22.060: I smdistributed/modelparallel/torch/model.py:137] [13] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 03:40:22.081: I smdistributed/modelparallel/torch/model.py:137] [4] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 03:40:22.081: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [13] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 03:40:22.087: I smdistributed/modelparallel/torch/model.py:137] [7] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 03:40:22.103: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [4] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 03:40:22.108: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [7] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 03:40:22.131 algo-2:226 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 03:40:22.152 algo-1:219 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 03:40:22.157 algo-1:222 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 03:40:22.255 algo-2:226 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 03:40:22.277 algo-1:219 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 03:40:22.281 algo-1:222 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:22.521: I smdistributed/modelparallel/torch/model.py:137] [0] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:22.542: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [0] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/200 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:22.591 algo-1:215 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:22.714 algo-1:215 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:22.722: W smdistributed/modelparallel/backend/split.py:167] Non-splittable object of type <class 'transformers.tokenization_utils_base.BatchEncoding'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:22.723: I smdistributed/modelparallel/torch/worker.py:300] Tracing on GPU. If the model parameters do not fit in a single GPU, you can set trace_device to `cpu`.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 03:40:22.949: I smdistributed/modelparallel/torch/model.py:137] [5] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 03:40:22.973: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [5] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 03:40:23.025 algo-1:220 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 03:40:23.158 algo-1:220 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 03:40:23.203: I smdistributed/modelparallel/torch/model.py:137] [1] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 03:40:23.225: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [1] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 03:40:23.248: I smdistributed/modelparallel/torch/model.py:137] [14] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 03:40:23.271: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [14] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 03:40:23.276 algo-1:216 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 03:40:23.325 algo-2:227 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 03:40:23.402 algo-1:216 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 03:40:23.460 algo-2:227 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 03:40:24.005: I smdistributed/modelparallel/torch/model.py:137] [11] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 03:40:24.027: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [11] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 03:40:24.076 algo-2:224 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 03:40:24.201 algo-2:224 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 03:40:24.457: I smdistributed/modelparallel/torch/model.py:137] [10] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 03:40:24.478: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [10] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 03:40:24.526 algo-2:223 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 03:40:24.648 algo-2:223 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 03:40:24.692: I smdistributed/modelparallel/torch/model.py:137] [15] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 03:40:24.712: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [15] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 03:40:24.760 algo-2:228 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 03:40:24.882 algo-2:228 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 03:40:25.776: I smdistributed/modelparallel/torch/model.py:137] [9] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 03:40:25.797: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [9] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 03:40:25.846 algo-2:222 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 03:40:25.967 algo-2:222 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:670] Partition assignments:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:679] main: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:679] main/module: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:679] main/module/module: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/lm_head: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/embed_tokens: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/norm: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/0: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/1: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/2: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/3: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/4: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/5: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/6: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/7: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/8: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/9: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/10: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/11: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/12: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/13: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/14: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/15: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/16: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/17: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/18: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/19: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/20: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/21: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/22: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/23: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/24: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/25: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/26: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/27: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/28: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/29: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/30: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/31: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 03:40:30.172: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 6 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 03:40:30.174: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 6 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 03:40:30.183: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 9 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 03:40:30.185: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 9 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 03:40:30.190: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 13 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 03:40:30.190: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 3 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 03:40:30.191: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 13 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 03:40:30.192: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 3 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 03:40:30.192: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 11 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 03:40:30.193: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 11 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 03:40:30.199: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 15 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 03:40:30.200: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 15 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 03:40:30.242: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 2 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 03:40:30.243: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 2 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 03:40:30.244: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 8 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 03:40:30.246: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 8 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 03:40:30.251: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 12 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 03:40:30.252: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 10 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 03:40:30.253: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 12 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 03:40:30.253: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 10 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 03:40:30.253: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 14 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 03:40:30.255: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 14 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 03:40:30.255: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 5 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 03:40:30.256: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 5 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 03:40:30.256: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 1 are 27. 27 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 03:40:30.258: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 1 are 3.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 03:40:30.260: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 7 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 03:40:30.261: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 7 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 03:40:30.261: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 4 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 03:40:30.262: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 4 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:38.778: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 0 are 12. 12 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:38.779: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 0 are 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:39.491: I smdistributed/modelparallel/torch/model.py:730] Finished partitioning the model\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:215 [0] NCCL INFO Bootstrap : Using eth0:10.0.248.37<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:215 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:215 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:215 [0] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:215 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:224 [3] NCCL INFO Bootstrap : Using eth0:10.0.209.56<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:223 [2] NCCL INFO Bootstrap : Using eth0:10.0.209.56<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:227 [6] NCCL INFO Bootstrap : Using eth0:10.0.209.56<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:228 [7] NCCL INFO Bootstrap : Using eth0:10.0.209.56<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:226 [5] NCCL INFO Bootstrap : Using eth0:10.0.209.56<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:225 [4] NCCL INFO Bootstrap : Using eth0:10.0.209.56<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:220 [5] NCCL INFO Bootstrap : Using eth0:10.0.248.37<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:219 [4] NCCL INFO Bootstrap : Using eth0:10.0.248.37<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:221 [0] NCCL INFO Bootstrap : Using eth0:10.0.209.56<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:222 [1] NCCL INFO Bootstrap : Using eth0:10.0.209.56<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:224 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:228 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:221 [6] NCCL INFO Bootstrap : Using eth0:10.0.248.37<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:217 [2] NCCL INFO Bootstrap : Using eth0:10.0.248.37<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:218 [3] NCCL INFO Bootstrap : Using eth0:10.0.248.37<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:222 [7] NCCL INFO Bootstrap : Using eth0:10.0.248.37<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:216 [1] NCCL INFO Bootstrap : Using eth0:10.0.248.37<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:223 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:223 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:227 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:227 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:224 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:228 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:228 [7] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:227 [6] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:227 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:224 [3] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:224 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:219 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:220 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:223 [2] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:223 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:220 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:219 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:228 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:220 [5] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:219 [4] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:220 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:219 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:222 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:225 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:221 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:226 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:226 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:222 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:225 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:221 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:225 [4] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:225 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:226 [5] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:226 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:222 [1] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:222 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:221 [0] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:221 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:222 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:222 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:216 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:222 [7] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:222 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:216 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:218 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:216 [1] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:218 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:216 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:218 [3] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:218 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:221 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:221 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:217 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:217 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:221 [6] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:221 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:217 [2] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:217 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:215 [0] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:215 [0] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:215 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:228 [7] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:228 [7] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:228 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:223 [2] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:223 [2] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:223 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:221 [0] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:221 [0] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:221 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:220 [5] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:219 [4] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:219 [4] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:220 [5] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:219 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:220 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:226 [5] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:222 [7] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:222 [7] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:222 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:226 [5] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:226 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:216 [1] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:216 [1] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:216 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:227 [6] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:227 [6] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:218 [3] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:218 [3] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:227 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:218 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:217 [2] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:221 [6] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:217 [2] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:221 [6] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:221 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:217 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:224 [3] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:224 [3] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:224 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:222 [1] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:225 [4] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:222 [1] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:222 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:225 [4] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:225 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO comm 0x7f0c840030d0 rank 0 nranks 1 cudaDev 0 busId 101c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO comm 0x7f33100030d0 rank 0 nranks 1 cudaDev 7 busId a01d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO comm 0x7f94d00030d0 rank 0 nranks 1 cudaDev 5 busId 901d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO comm 0x7f7a040030d0 rank 0 nranks 1 cudaDev 7 busId a01d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO comm 0x7f72d00030d0 rank 0 nranks 1 cudaDev 0 busId 101c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO comm 0x7f102c0030d0 rank 0 nranks 1 cudaDev 5 busId 901d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO comm 0x7f55d00030d0 rank 0 nranks 1 cudaDev 4 busId 901c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO comm 0x7f9d700030d0 rank 0 nranks 1 cudaDev 6 busId a01c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO comm 0x7fcd600030d0 rank 0 nranks 1 cudaDev 2 busId 201c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO comm 0x7f8a900030d0 rank 0 nranks 1 cudaDev 1 busId 101d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO comm 0x7efee00030d0 rank 0 nranks 1 cudaDev 3 busId 201d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO comm 0x7f20100030d0 rank 0 nranks 1 cudaDev 4 busId 901c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO comm 0x7fa3d00030d0 rank 0 nranks 1 cudaDev 2 busId 201c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO comm 0x7ef8980030d0 rank 0 nranks 1 cudaDev 6 busId a01c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO comm 0x7ff2740030d0 rank 0 nranks 1 cudaDev 1 busId 101d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO comm 0x7fce280030d0 rank 0 nranks 1 cudaDev 3 busId 201d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [2] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [0] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [3] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [4] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [6] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [5] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [7] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [14] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [10] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [15] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [11] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 03:41:14.129: I smdistributed/modelparallel/torch/ddp_model.py:632] [9] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 03:41:14.129: I smdistributed/modelparallel/torch/ddp_model.py:632] [12] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 03:41:14.129: I smdistributed/modelparallel/torch/ddp_model.py:632] [8] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 03:41:14.129: I smdistributed/modelparallel/torch/ddp_model.py:632] [13] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 03:41:14.129: I smdistributed/modelparallel/torch/ddp_model.py:632] [1] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 0/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] 0/-1/-1->7->6\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] 9/-1/-1->8->15 [2] 9/0/-1->8->-1 [3] 9/-1/-1->8->15\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] -1/-1/-1->9->8 [2] 10/-1/-1->9->8 [3] -1/-1/-1->9->8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->2 [2] 11/-1/-1->10->9 [3] 11/2/-1->10->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11 [2] 13/-1/-1->12->11 [3] 13/-1/-1->12->11\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10 [2] 12/-1/-1->11->10 [3] 12/-1/-1->11->10\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] 14/-1/-1->13->12 [3] 14/-1/-1->13->12\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->13\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] 8/-1/-1->15->14 [2] -1/-1/-1->15->14 [3] 8/-1/-1->15->14\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 00/04 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/10/-1->2->-1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->10\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] -1/-1/-1->1->0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 01/04 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 02/04 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 03/04 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/-1/-1->0->7 [2] 1/-1/-1->0->8 [3] 1/-1/-1->0->7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 01 : 8[101c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 01 : 2[201c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 01 : 0[101c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 01 : 10[201c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Channel 01 : 12[901c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 03 : 8[101c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 03 : 0[101c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 03 : 2[201c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 03 : 10[201c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Channel 03 : 12[901c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Channel 01 : 4[901c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Channel 00 : 9[101d0] -> 0[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Channel 00 : 1[101d0] -> 8[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Channel 03 : 4[901c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Channel 00 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Channel 00 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Channel 00 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Channel 01 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Channel 01 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Channel 02 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Channel 02 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Channel 01 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Channel 03 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Channel 03 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Channel 00 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Channel 02 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Channel 01 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Channel 03 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Channel 02 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Channel 02 : 9[101d0] -> 0[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Channel 03 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Channel 01 : 11[201d0] -> 2[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Channel 02 : 1[101d0] -> 8[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 00 : 1[101d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 01 : 3[201d0] -> 10[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 00 : 9[101d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Channel 01 : 3[201d0] -> 10[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 01 : 11[201d0] -> 2[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Channel 03 : 11[201d0] -> 2[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Channel 03 : 3[201d0] -> 10[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 02 : 9[101d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 02 : 1[101d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 03 : 3[201d0] -> 10[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 00 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 00 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 02 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 02 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO Channel 00 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO Channel 00 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO Channel 01 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO Channel 01 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO Channel 02 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 03 : 11[201d0] -> 2[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO Channel 02 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO Channel 03 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO Channel 03 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Channel 00 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Channel 01 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Channel 00 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Channel 02 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Channel 01 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Channel 03 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Channel 02 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Channel 03 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Channel 00 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Channel 01 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Channel 02 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Channel 03 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Channel 00 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Channel 00 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Channel 02 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Channel 02 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Channel 01 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Channel 03 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Channel 00 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Channel 02 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Channel 00 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Channel 00 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Channel 01 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Channel 01 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Channel 02 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Channel 02 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Channel 03 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Channel 03 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Channel 00 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Channel 01 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Channel 00 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Channel 02 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Channel 01 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Channel 03 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Channel 02 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Channel 03 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 00 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 01 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 02 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 00 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 03 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 02 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 00 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 02 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 00 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 01 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Channel 00 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 02 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Channel 00 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Channel 02 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 03 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Channel 01 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 00 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Channel 02 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 00 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Channel 03 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 01 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 01 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 02 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Channel 00 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 02 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 03 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 03 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Channel 01 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Channel 02 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Channel 00 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Channel 00 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Channel 03 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Channel 02 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Channel 02 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 01 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Channel 01 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 03 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Channel 03 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Channel 01 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 01 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Channel 03 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO Channel 01 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 03 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO Channel 03 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Channel 00 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 01 : 10[201c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Channel 02 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Channel 01 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Channel 03 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO Channel 01 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Channel 01 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO Channel 03 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Channel 03 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 00 : 8[101c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 01 : 2[201c0] -> 10[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 03 : 10[201c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 00 : 0[101c0] -> 8[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 02 : 8[101c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 03 : 2[201c0] -> 10[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 01 : 2[201c0] -> 10[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 02 : 0[101c0] -> 8[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 01 : 10[201c0] -> 2[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 00 : 0[101c0] -> 8[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 03 : 2[201c0] -> 10[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 00 : 8[101c0] -> 0[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 03 : 10[201c0] -> 2[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 02 : 0[101c0] -> 8[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 02 : 8[101c0] -> 0[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO comm 0x7f9c180030d0 rank 14 nranks 16 cudaDev 6 busId a01c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO comm 0x7f0ebc0030d0 rank 13 nranks 16 cudaDev 5 busId 901d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO comm 0x7efd640030d0 rank 11 nranks 16 cudaDev 3 busId 201d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO comm 0x7f30b80030d0 rank 15 nranks 16 cudaDev 7 busId a01d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO comm 0x7f1dbc0030d0 rank 12 nranks 16 cudaDev 4 busId 901c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO comm 0x7fcbe40030d0 rank 10 nranks 16 cudaDev 2 busId 201c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO comm 0x7f89040030d0 rank 9 nranks 16 cudaDev 1 busId 101d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO comm 0x7f714c0030d0 rank 8 nranks 16 cudaDev 0 busId 101c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO comm 0x7fa06c0030d0 rank 2 nranks 16 cudaDev 2 busId 201c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO comm 0x7f072c0030d0 rank 0 nranks 16 cudaDev 0 busId 101c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO comm 0x7f78280030d0 rank 7 nranks 16 cudaDev 7 busId a01d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO comm 0x7fcc740030d0 rank 3 nranks 16 cudaDev 3 busId 201d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO comm 0x7ef50c0030d0 rank 6 nranks 16 cudaDev 6 busId a01c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO comm 0x7fedf00030d0 rank 1 nranks 16 cudaDev 1 busId 101d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO comm 0x7f92fc0030d0 rank 5 nranks 16 cudaDev 5 busId 901d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO comm 0x7f54100030d0 rank 4 nranks 16 cudaDev 4 busId 901c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:215 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  0%|          | 1/200 [00:55<3:03:03, 55.19s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 1/200 [00:53<2:58:58, 53.96s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  1%|          | 2/200 [01:14<1:52:20, 34.04s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 2/200 [01:13<1:50:40, 33.54s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  2%|▏         | 3/200 [01:33<1:28:54, 27.08s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 3/200 [01:31<1:28:00, 26.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  2%|▏         | 4/200 [01:52<1:17:47, 23.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 4/200 [01:50<1:17:14, 23.65s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  2%|▎         | 5/200 [02:10<1:11:32, 22.01s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|▎         | 5/200 [02:09<1:11:11, 21.91s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  3%|▎         | 6/200 [02:29<1:07:37, 20.91s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 6/200 [02:28<1:07:23, 20.84s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  4%|▎         | 7/200 [02:48<1:05:02, 20.22s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|▎         | 7/200 [02:47<1:04:53, 20.17s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  4%|▍         | 8/200 [03:07<1:03:14, 19.76s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 8/200 [03:05<1:03:08, 19.73s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  4%|▍         | 9/200 [03:25<1:01:56, 19.46s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 9/200 [03:24<1:01:52, 19.44s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  5%|▌         | 10/200 [03:44<1:00:58, 19.25s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  5%|▌         | 10/200 [03:43<1:00:55, 19.24s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  6%|▌         | 11/200 [04:03<1:00:11, 19.11s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 11/200 [04:02<1:00:09, 19.10s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  6%|▌         | 12/200 [04:22<59:33, 19.01s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 12/200 [04:21<59:31, 19.00s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  6%|▋         | 13/200 [04:41<59:01, 18.94s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▋         | 13/200 [04:39<59:00, 18.93s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  7%|▋         | 14/200 [04:59<58:34, 18.90s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 14/200 [04:58<58:33, 18.89s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  8%|▊         | 15/200 [05:18<58:09, 18.86s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 15/200 [05:17<58:09, 18.86s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  8%|▊         | 16/200 [05:37<57:45, 18.84s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 16/200 [05:36<57:45, 18.83s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  8%|▊         | 17/200 [05:56<57:25, 18.83s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 17/200 [05:55<57:24, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  9%|▉         | 18/200 [06:15<57:04, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  9%|▉         | 18/200 [06:13<57:04, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 10%|▉         | 19/200 [06:33<56:45, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|▉         | 19/200 [06:32<56:45, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 10%|█         | 20/200 [06:52<56:25, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|█         | 20/200 [06:51<56:25, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 10%|█         | 21/200 [07:11<56:06, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|█         | 21/200 [07:10<56:06, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 11%|█         | 22/200 [07:30<55:46, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 11%|█         | 22/200 [07:29<55:46, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 12%|█▏        | 23/200 [07:49<55:26, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 23/200 [07:47<55:25, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 12%|█▏        | 24/200 [08:07<55:07, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 24/200 [08:06<55:07, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 12%|█▎        | 25/200 [08:26<54:48, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▎        | 25/200 [08:25<54:48, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 13%|█▎        | 26/200 [08:45<54:29, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 26/200 [08:44<54:29, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 14%|█▎        | 27/200 [09:04<54:11, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▎        | 27/200 [09:02<54:11, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 14%|█▍        | 28/200 [09:23<53:53, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▍        | 28/200 [09:21<53:53, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 14%|█▍        | 29/200 [09:41<53:34, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▍        | 29/200 [09:40<53:34, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 15%|█▌        | 30/200 [10:00<53:15, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▌        | 30/200 [09:59<53:15, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 16%|█▌        | 31/200 [10:19<52:56, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▌        | 31/200 [10:18<52:56, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 16%|█▌        | 32/200 [10:38<52:37, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▌        | 32/200 [10:36<52:37, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 16%|█▋        | 33/200 [10:57<52:19, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▋        | 33/200 [10:55<52:19, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 17%|█▋        | 34/200 [11:15<52:00, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 34/200 [11:14<52:00, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 18%|█▊        | 35/200 [11:34<51:41, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 35/200 [11:33<51:41, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 18%|█▊        | 36/200 [11:53<51:22, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 36/200 [11:52<51:22, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 18%|█▊        | 37/200 [12:12<51:02, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 37/200 [12:10<51:02, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 19%|█▉        | 38/200 [12:30<50:43, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▉        | 38/200 [12:29<50:43, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 20%|█▉        | 39/200 [12:49<50:24, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|█▉        | 39/200 [12:48<50:24, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 20%|██        | 40/200 [13:08<50:06, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 40/200 [13:07<50:06, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 20%|██        | 41/200 [13:27<49:48, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 41/200 [13:26<49:48, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 21%|██        | 42/200 [13:46<49:29, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 21%|██        | 42/200 [13:44<49:29, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 22%|██▏       | 43/200 [14:04<49:10, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 43/200 [14:03<49:10, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 22%|██▏       | 44/200 [14:23<48:51, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 44/200 [14:22<48:51, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 22%|██▎       | 45/200 [14:42<48:33, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▎       | 45/200 [14:41<48:33, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 23%|██▎       | 46/200 [15:01<48:13, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 46/200 [15:00<48:13, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 24%|██▎       | 47/200 [15:20<47:55, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▎       | 47/200 [15:18<47:55, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 24%|██▍       | 48/200 [15:38<47:36, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▍       | 48/200 [15:37<47:36, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 24%|██▍       | 49/200 [15:57<47:17, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▍       | 49/200 [15:56<47:17, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 25%|██▌       | 50/200 [16:16<46:59, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▌       | 50/200 [16:15<46:59, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 00/04 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 01/04 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] -1/-1/-1->1->0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 02/04 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 03/04 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/-1/-1->0->7 [2] 1/-1/-1->0->8 [3] 1/-1/-1->0->7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/10/-1->2->-1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 0/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] 0/-1/-1->7->6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] 9/-1/-1->8->15 [2] 9/0/-1->8->-1 [3] 9/-1/-1->8->15\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11 [2] 13/-1/-1->12->11 [3] 13/-1/-1->12->11\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] 8/-1/-1->15->14 [2] -1/-1/-1->15->14 [3] 8/-1/-1->15->14\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] 14/-1/-1->13->12 [3] 14/-1/-1->13->12\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] -1/-1/-1->9->8 [2] 10/-1/-1->9->8 [3] -1/-1/-1->9->8\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->2 [2] 11/-1/-1->10->9 [3] 11/2/-1->10->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->13\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10 [2] 12/-1/-1->11->10 [3] 12/-1/-1->11->10\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 01 : 0[101c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 01 : 2[201c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 01 : 8[101c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 03 : 0[101c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 01 : 10[201c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 03 : 2[201c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Channel 01 : 12[901c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 03 : 8[101c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 03 : 10[201c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Channel 00 : 1[101d0] -> 8[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Channel 03 : 12[901c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Channel 01 : 4[901c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Channel 00 : 9[101d0] -> 0[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Channel 03 : 4[901c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Channel 00 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Channel 01 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Channel 00 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Channel 00 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Channel 00 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Channel 02 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Channel 01 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Channel 01 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Channel 02 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 00 : 9[101d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Channel 02 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Channel 01 : 3[201d0] -> 10[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Channel 03 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Channel 03 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Channel 01 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Channel 03 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Channel 02 : 1[101d0] -> 8[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Channel 02 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Channel 03 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 01 : 11[201d0] -> 2[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 00 : 1[101d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Channel 02 : 9[101d0] -> 0[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Channel 01 : 11[201d0] -> 2[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 01 : 3[201d0] -> 10[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Channel 03 : 3[201d0] -> 10[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 02 : 9[101d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 00 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 02 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 03 : 11[201d0] -> 2[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 02 : 1[101d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Channel 03 : 11[201d0] -> 2[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 00 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO Channel 00 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO Channel 01 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 02 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 03 : 3[201d0] -> 10[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO Channel 02 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO Channel 03 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 00 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO Channel 00 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 02 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO Channel 01 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 00 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO Channel 02 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Channel 00 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 02 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO Channel 03 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Channel 00 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Channel 02 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Channel 02 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Channel 01 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Channel 00 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Channel 01 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Channel 03 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Channel 02 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Channel 03 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Channel 00 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Channel 02 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 00 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 00 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 01 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Channel 00 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 01 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 02 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Channel 01 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 02 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 03 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 00 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Channel 02 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 03 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 00 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 01 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Channel 03 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 01 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 02 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Channel 00 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Channel 00 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Channel 00 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 02 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Channel 02 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 03 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Channel 00 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Channel 01 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Channel 02 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 03 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Channel 01 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Channel 00 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Channel 02 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Channel 00 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Channel 02 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Channel 01 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Channel 03 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Channel 00 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Channel 00 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Channel 01 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Channel 03 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Channel 02 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Channel 01 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Channel 00 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Channel 02 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 01 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Channel 01 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Channel 03 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Channel 01 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Channel 02 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 03 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 01 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Channel 03 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Channel 00 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Channel 02 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Channel 03 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 03 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Channel 02 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Channel 00 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Channel 03 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Channel 02 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Channel 03 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO Channel 01 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Channel 02 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO Channel 03 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 00 : 8[101c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO Channel 01 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 01 : 2[201c0] -> 10[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO Channel 03 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Channel 01 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Channel 01 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 01 : 10[201c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Channel 01 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Channel 03 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Channel 03 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Channel 01 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Channel 03 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 00 : 0[101c0] -> 8[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Channel 03 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 02 : 8[101c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 03 : 2[201c0] -> 10[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 03 : 10[201c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 02 : 0[101c0] -> 8[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 00 : 0[101c0] -> 8[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 01 : 10[201c0] -> 2[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 01 : 2[201c0] -> 10[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 00 : 8[101c0] -> 0[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 02 : 0[101c0] -> 8[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 03 : 10[201c0] -> 2[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 03 : 2[201c0] -> 10[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 02 : 8[101c0] -> 0[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO comm 0x7f9c18007a90 rank 14 nranks 16 cudaDev 6 busId a01c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO comm 0x7efd64007a90 rank 11 nranks 16 cudaDev 3 busId 201d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO comm 0x7fcbe4007cc0 rank 10 nranks 16 cudaDev 2 busId 201c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO comm 0x7f30b8007a90 rank 15 nranks 16 cudaDev 7 busId a01d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO comm 0x7f0ebc007a90 rank 13 nranks 16 cudaDev 5 busId 901d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO comm 0x7f8904007a90 rank 9 nranks 16 cudaDev 1 busId 101d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO comm 0x7f714c007cc0 rank 8 nranks 16 cudaDev 0 busId 101c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO comm 0x7f1dbc007a90 rank 12 nranks 16 cudaDev 4 busId 901c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO comm 0x7fa06c007cc0 rank 2 nranks 16 cudaDev 2 busId 201c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO comm 0x7f92fc007a90 rank 5 nranks 16 cudaDev 5 busId 901d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO comm 0x7fedf0007a90 rank 1 nranks 16 cudaDev 1 busId 101d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO comm 0x7f7828007a90 rank 7 nranks 16 cudaDev 7 busId a01d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO comm 0x7ef50c007a90 rank 6 nranks 16 cudaDev 6 busId a01c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO comm 0x7fcc74007a90 rank 3 nranks 16 cudaDev 3 busId 201d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO comm 0x7f06680030d0 rank 0 nranks 16 cudaDev 0 busId 101c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO comm 0x7f5410007a90 rank 4 nranks 16 cudaDev 4 busId 901c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:215 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:{'loss': 4.3493, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015                                                #015[1,mpirank:0,algo-1]<stderr>:#015 25%|██▌       | 50/200 [16:17<46:59, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:{'loss': 4.3493, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 25%|██▌       | 50/200 [16:18<46:59, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 26%|██▌       | 51/200 [16:37<48:24, 19.50s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 51/200 [16:36<48:24, 19.50s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 26%|██▌       | 52/200 [16:56<47:34, 19.29s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 52/200 [16:55<47:34, 19.29s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 26%|██▋       | 53/200 [17:15<46:54, 19.14s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▋       | 53/200 [17:14<46:54, 19.14s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 27%|██▋       | 54/200 [17:34<46:19, 19.04s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 54/200 [17:32<46:19, 19.04s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 28%|██▊       | 55/200 [17:52<45:50, 18.97s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 55/200 [17:51<45:50, 18.97s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 28%|██▊       | 56/200 [18:11<45:24, 18.92s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 56/200 [18:10<45:24, 18.92s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 28%|██▊       | 57/200 [18:30<44:59, 18.88s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 57/200 [18:29<44:59, 18.88s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 29%|██▉       | 58/200 [18:49<44:36, 18.85s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▉       | 58/200 [18:47<44:36, 18.85s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 30%|██▉       | 59/200 [19:07<44:15, 18.83s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|██▉       | 59/200 [19:06<44:15, 18.83s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 30%|███       | 60/200 [19:26<43:54, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|███       | 60/200 [19:25<43:54, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 30%|███       | 61/200 [19:45<43:34, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|███       | 61/200 [19:44<43:34, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 31%|███       | 62/200 [20:04<43:14, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 62/200 [20:03<43:14, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 32%|███▏      | 63/200 [20:23<42:56, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 63/200 [20:21<42:56, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 32%|███▏      | 64/200 [20:41<42:37, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 64/200 [20:40<42:37, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 32%|███▎      | 65/200 [21:00<42:17, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▎      | 65/200 [20:59<42:17, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 33%|███▎      | 66/200 [21:19<41:58, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 66/200 [21:18<41:58, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 34%|███▎      | 67/200 [21:38<41:38, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▎      | 67/200 [21:37<41:38, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 34%|███▍      | 68/200 [21:57<41:20, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▍      | 68/200 [21:55<41:20, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 34%|███▍      | 69/200 [22:15<41:01, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▍      | 69/200 [22:14<41:01, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 35%|███▌      | 70/200 [22:34<40:42, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▌      | 70/200 [22:33<40:42, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 36%|███▌      | 71/200 [22:53<40:23, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 71/200 [22:52<40:23, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 36%|███▌      | 72/200 [23:12<40:05, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 72/200 [23:11<40:05, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 36%|███▋      | 73/200 [23:31<39:46, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▋      | 73/200 [23:29<39:46, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 37%|███▋      | 74/200 [23:49<39:27, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 74/200 [23:48<39:27, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 38%|███▊      | 75/200 [24:08<39:09, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 75/200 [24:07<39:09, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 38%|███▊      | 76/200 [24:27<38:50, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 76/200 [24:26<38:50, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 38%|███▊      | 77/200 [24:46<38:31, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 77/200 [24:44<38:31, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 39%|███▉      | 78/200 [25:05<38:13, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▉      | 78/200 [25:03<38:13, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 40%|███▉      | 79/200 [25:23<37:54, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|███▉      | 79/200 [25:22<37:54, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 40%|████      | 80/200 [25:42<37:36, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 80/200 [25:41<37:36, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 40%|████      | 81/200 [26:01<37:17, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 81/200 [26:00<37:17, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 41%|████      | 82/200 [26:20<36:58, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 82/200 [26:19<36:58, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 42%|████▏     | 83/200 [26:39<36:39, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 83/200 [26:37<36:39, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 42%|████▏     | 84/200 [26:57<36:20, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 84/200 [26:56<36:20, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 42%|████▎     | 85/200 [27:16<36:01, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▎     | 85/200 [27:15<36:01, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 43%|████▎     | 86/200 [27:35<35:41, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 86/200 [27:34<35:41, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 44%|████▎     | 87/200 [27:54<35:22, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▎     | 87/200 [27:52<35:22, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 44%|████▍     | 88/200 [28:12<35:03, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▍     | 88/200 [28:11<35:03, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 44%|████▍     | 89/200 [28:31<34:44, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▍     | 89/200 [28:30<34:44, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 45%|████▌     | 90/200 [28:50<34:25, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▌     | 90/200 [28:49<34:25, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 46%|████▌     | 91/200 [29:09<34:06, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 91/200 [29:08<34:06, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 46%|████▌     | 92/200 [29:28<33:48, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 92/200 [29:26<33:48, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 46%|████▋     | 93/200 [29:46<33:29, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▋     | 93/200 [29:45<33:29, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 47%|████▋     | 94/200 [30:05<33:10, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 94/200 [30:04<33:10, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 48%|████▊     | 95/200 [30:24<32:51, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 95/200 [30:23<32:51, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 48%|████▊     | 96/200 [30:43<32:32, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 96/200 [30:41<32:32, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 48%|████▊     | 97/200 [31:01<32:13, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 97/200 [31:00<32:13, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 49%|████▉     | 98/200 [31:20<31:55, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▉     | 98/200 [31:19<31:55, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 50%|████▉     | 99/200 [31:39<31:36, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|████▉     | 99/200 [31:38<31:36, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 50%|█████     | 100/200 [31:58<31:17, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|█████     | 100/200 [31:57<31:17, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:{'loss': 0.1956, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|█████     | 100/200 [31:57<31:17, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:{'loss': 0.1956, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015                                                 #015[1,mpirank:8,algo-2]<stderr>:#015 50%|█████     | 100/200 [31:58<31:17, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 50%|█████     | 101/200 [32:17<30:59, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|█████     | 101/200 [32:15<30:59, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 51%|█████     | 102/200 [32:35<30:40, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████     | 102/200 [32:34<30:40, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 52%|█████▏    | 103/200 [32:54<30:22, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 103/200 [32:53<30:22, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 52%|█████▏    | 104/200 [33:13<30:03, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 104/200 [33:12<30:03, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 52%|█████▎    | 105/200 [33:32<29:45, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▎    | 105/200 [33:30<29:45, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 53%|█████▎    | 106/200 [33:51<29:26, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 106/200 [33:49<29:26, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 54%|█████▎    | 107/200 [34:09<29:07, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▎    | 107/200 [34:08<29:07, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 54%|█████▍    | 108/200 [34:28<28:48, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 108/200 [34:27<28:48, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 55%|█████▍    | 109/200 [34:47<28:29, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▍    | 109/200 [34:46<28:29, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 55%|█████▌    | 110/200 [35:06<28:10, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▌    | 110/200 [35:04<28:10, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 56%|█████▌    | 111/200 [35:24<27:52, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 111/200 [35:23<27:52, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 56%|█████▌    | 112/200 [35:43<27:34, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 112/200 [35:42<27:34, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 56%|█████▋    | 113/200 [36:02<27:14, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▋    | 113/200 [36:01<27:14, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 57%|█████▋    | 114/200 [36:21<26:56, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 114/200 [36:20<26:56, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 57%|█████▊    | 115/200 [36:40<26:37, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▊    | 115/200 [36:38<26:37, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 58%|█████▊    | 116/200 [36:58<26:19, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 116/200 [36:57<26:19, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 58%|█████▊    | 117/200 [37:17<26:00, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 117/200 [37:16<26:00, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 59%|█████▉    | 118/200 [37:36<25:41, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▉    | 118/200 [37:35<25:41, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 60%|█████▉    | 119/200 [37:55<25:21, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|█████▉    | 119/200 [37:54<25:21, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 60%|██████    | 120/200 [38:14<25:03, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 120/200 [38:12<25:03, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 60%|██████    | 121/200 [38:32<24:44, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 121/200 [38:31<24:44, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 61%|██████    | 122/200 [38:51<24:25, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 122/200 [38:50<24:25, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 62%|██████▏   | 123/200 [39:10<24:06, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 123/200 [39:09<24:06, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 62%|██████▏   | 124/200 [39:29<23:47, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 124/200 [39:28<23:47, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 62%|██████▎   | 125/200 [39:48<23:28, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▎   | 125/200 [39:46<23:28, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 63%|██████▎   | 126/200 [40:06<23:10, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 126/200 [40:05<23:10, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 64%|██████▎   | 127/200 [40:25<22:51, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▎   | 127/200 [40:24<22:51, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 64%|██████▍   | 128/200 [40:44<22:32, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 128/200 [40:43<22:32, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 64%|██████▍   | 129/200 [41:03<22:14, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 129/200 [41:01<22:14, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 65%|██████▌   | 130/200 [41:22<21:55, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▌   | 130/200 [41:20<21:55, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 66%|██████▌   | 131/200 [41:40<21:36, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▌   | 131/200 [41:39<21:36, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 66%|██████▌   | 132/200 [41:59<21:17, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▌   | 132/200 [41:58<21:17, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 66%|██████▋   | 133/200 [42:18<20:59, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▋   | 133/200 [42:17<20:59, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 67%|██████▋   | 134/200 [42:37<20:39, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 134/200 [42:35<20:39, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 68%|██████▊   | 135/200 [42:55<20:21, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 135/200 [42:54<20:21, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 68%|██████▊   | 136/200 [43:14<20:02, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 136/200 [43:13<20:02, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 68%|██████▊   | 137/200 [43:33<19:43, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 137/200 [43:32<19:43, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 69%|██████▉   | 138/200 [43:52<19:25, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 138/200 [43:51<19:25, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 70%|██████▉   | 139/200 [44:11<19:06, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|██████▉   | 139/200 [44:09<19:06, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 70%|███████   | 140/200 [44:29<18:47, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|███████   | 140/200 [44:28<18:47, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 70%|███████   | 141/200 [44:48<18:28, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|███████   | 141/200 [44:47<18:28, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 71%|███████   | 142/200 [45:07<18:10, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████   | 142/200 [45:06<18:10, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 72%|███████▏  | 143/200 [45:26<17:51, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 143/200 [45:25<17:51, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 72%|███████▏  | 144/200 [45:45<17:32, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 144/200 [45:43<17:32, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 72%|███████▎  | 145/200 [46:03<17:13, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▎  | 145/200 [46:02<17:13, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 73%|███████▎  | 146/200 [46:22<16:55, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 146/200 [46:21<16:55, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 74%|███████▎  | 147/200 [46:41<16:36, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▎  | 147/200 [46:40<16:36, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 74%|███████▍  | 148/200 [47:00<16:17, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 148/200 [46:59<16:17, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 74%|███████▍  | 149/200 [47:19<15:58, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 149/200 [47:17<15:58, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 75%|███████▌  | 150/200 [47:38<15:41, 18.83s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▌  | 150/200 [47:36<15:41, 18.83s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:{'loss': 0.2894, 'learning_rate': 3e-06, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015                                                 #015[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▌  | 150/200 [47:36<15:41, 18.83s/it][1,mpirank:8,algo-2]<stderr>:#015 75%|███████▌  | 150/200 [47:38<15:41, 18.83s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:{'loss': 0.2894, 'learning_rate': 3e-06, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 76%|███████▌  | 151/200 [47:56<15:22, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 151/200 [47:55<15:22, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 76%|███████▌  | 152/200 [48:15<15:03, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 152/200 [48:14<15:03, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 76%|███████▋  | 153/200 [48:34<14:44, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▋  | 153/200 [48:33<14:44, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 77%|███████▋  | 154/200 [48:53<14:25, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 154/200 [48:51<14:25, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 78%|███████▊  | 155/200 [49:12<14:06, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 155/200 [49:10<14:06, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 78%|███████▊  | 156/200 [49:30<13:47, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 156/200 [49:29<13:47, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 78%|███████▊  | 157/200 [49:49<13:28, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 157/200 [49:48<13:28, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 79%|███████▉  | 158/200 [50:08<13:10, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 158/200 [50:07<13:10, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 80%|███████▉  | 159/200 [50:27<12:51, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|███████▉  | 159/200 [50:26<12:51, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 80%|████████  | 160/200 [50:46<12:32, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 160/200 [50:44<12:32, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 80%|████████  | 161/200 [51:04<12:13, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 161/200 [51:03<12:13, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 81%|████████  | 162/200 [51:23<11:54, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 162/200 [51:22<11:54, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 82%|████████▏ | 163/200 [51:42<11:35, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 163/200 [51:41<11:35, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 82%|████████▏ | 164/200 [52:01<11:16, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 164/200 [52:00<11:16, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 82%|████████▎ | 165/200 [52:20<10:58, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▎ | 165/200 [52:18<10:58, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 83%|████████▎ | 166/200 [52:38<10:39, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 166/200 [52:37<10:39, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 84%|████████▎ | 167/200 [52:57<10:20, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▎ | 167/200 [52:56<10:20, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 84%|████████▍ | 168/200 [53:16<10:01, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▍ | 168/200 [53:15<10:01, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 84%|████████▍ | 169/200 [53:35<09:42, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▍ | 169/200 [53:33<09:42, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 85%|████████▌ | 170/200 [53:53<09:23, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▌ | 170/200 [53:52<09:23, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 86%|████████▌ | 171/200 [54:12<09:04, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▌ | 171/200 [54:11<09:04, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 86%|████████▌ | 172/200 [54:31<08:46, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▌ | 172/200 [54:30<08:46, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 86%|████████▋ | 173/200 [54:50<08:27, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▋ | 173/200 [54:49<08:27, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 87%|████████▋ | 174/200 [55:09<08:08, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 174/200 [55:07<08:08, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 88%|████████▊ | 175/200 [55:27<07:49, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 175/200 [55:26<07:49, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 88%|████████▊ | 176/200 [55:46<07:30, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 176/200 [55:45<07:30, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 88%|████████▊ | 177/200 [56:05<07:12, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 177/200 [56:04<07:12, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 89%|████████▉ | 178/200 [56:24<06:53, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▉ | 178/200 [56:23<06:53, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 90%|████████▉ | 179/200 [56:43<06:34, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|████████▉ | 179/200 [56:41<06:34, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 90%|█████████ | 180/200 [57:01<06:15, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|█████████ | 180/200 [57:00<06:15, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 90%|█████████ | 181/200 [57:20<05:57, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|█████████ | 181/200 [57:19<05:57, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 91%|█████████ | 182/200 [57:39<05:38, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████ | 182/200 [57:38<05:38, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 92%|█████████▏| 183/200 [57:58<05:19, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 183/200 [57:57<05:19, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 92%|█████████▏| 184/200 [58:17<05:00, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 184/200 [58:15<05:00, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 92%|█████████▎| 185/200 [58:35<04:41, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▎| 185/200 [58:34<04:41, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 93%|█████████▎| 186/200 [58:54<04:22, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 186/200 [58:53<04:22, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 94%|█████████▎| 187/200 [59:13<04:04, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▎| 187/200 [59:12<04:04, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 94%|█████████▍| 188/200 [59:32<03:45, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 188/200 [59:30<03:45, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 94%|█████████▍| 189/200 [59:50<03:26, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 189/200 [59:49<03:26, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 95%|█████████▌| 190/200 [1:00:09<03:07, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▌| 190/200 [1:00:08<03:07, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 96%|█████████▌| 191/200 [1:00:28<02:49, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 191/200 [1:00:27<02:49, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 96%|█████████▌| 192/200 [1:00:47<02:30, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 192/200 [1:00:46<02:30, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 96%|█████████▋| 193/200 [1:01:06<02:11, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▋| 193/200 [1:01:04<02:11, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 97%|█████████▋| 194/200 [1:01:24<01:52, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 194/200 [1:01:23<01:52, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 98%|█████████▊| 195/200 [1:01:43<01:33, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 195/200 [1:01:42<01:33, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 98%|█████████▊| 196/200 [1:02:02<01:15, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 196/200 [1:02:01<01:15, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 98%|█████████▊| 197/200 [1:02:21<00:56, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 197/200 [1:02:19<00:56, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 99%|█████████▉| 198/200 [1:02:39<00:37, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 198/200 [1:02:38<00:37, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015100%|█████████▉| 199/200 [1:02:58<00:18, 18.76s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|█████████▉| 199/200 [1:02:57<00:18, 18.76s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015100%|██████████| 200/200 [1:03:17<00:00, 18.76s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 200/200 [1:03:16<00:00, 18.76s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:{'loss': 0.951, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 200/200 [1:03:16<00:00, 18.76s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:{'loss': 0.951, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015100%|██████████| 200/200 [1:03:17<00:00, 18.76s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:{'train_runtime': 3796.2397, 'train_samples_per_second': 0.421, 'train_steps_per_second': 0.053, 'train_loss': 1.4463429260253906, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 200/200 [1:03:16<00:00, 18.76s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:{'train_runtime': 3797.4775, 'train_samples_per_second': 0.421, 'train_steps_per_second': 0.053, 'train_loss': 1.4463429260253906, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015100%|██████████| 200/200 [1:03:17<00:00, 18.76s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 200/200 [1:03:16<00:00, 18.98s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015100%|██████████| 200/200 [1:03:17<00:00, 18.99s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 04:43:39.361: I smdistributed/modelparallel/torch/model.py:907] [14] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 04:43:39.361: W smdistributed/modelparallel/torch/model.py:917] [14] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 04:43:39.365: I smdistributed/modelparallel/torch/model.py:907] [3] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 04:43:39.365: W smdistributed/modelparallel/torch/model.py:917] [3] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 04:43:39.367: I smdistributed/modelparallel/torch/model.py:907] [4] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 04:43:39.367: W smdistributed/modelparallel/torch/model.py:917] [4] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 04:43:39.368: I smdistributed/modelparallel/torch/model.py:907] [5] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 04:43:39.368: W smdistributed/modelparallel/torch/model.py:917] [5] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 04:43:39.369: I smdistributed/modelparallel/torch/model.py:907] [9] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 04:43:39.370: W smdistributed/modelparallel/torch/model.py:917] [9] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 04:43:39.373: I smdistributed/modelparallel/torch/model.py:907] [6] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 04:43:39.373: W smdistributed/modelparallel/torch/model.py:917] [6] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 04:43:39.380: I smdistributed/modelparallel/torch/model.py:907] [13] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 04:43:39.380: W smdistributed/modelparallel/torch/model.py:917] [13] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 04:43:39.381: I smdistributed/modelparallel/torch/model.py:907] [12] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 04:43:39.382: W smdistributed/modelparallel/torch/model.py:917] [12] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 04:43:39.385: I smdistributed/modelparallel/torch/model.py:907] [11] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 04:43:39.385: W smdistributed/modelparallel/torch/model.py:917] [11] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 04:43:39.385: I smdistributed/modelparallel/torch/model.py:907] [15] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 04:43:39.385: W smdistributed/modelparallel/torch/model.py:917] [15] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 04:43:39.393: I smdistributed/modelparallel/torch/model.py:907] [2] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 04:43:39.393: W smdistributed/modelparallel/torch/model.py:917] [2] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 04:43:39.402: I smdistributed/modelparallel/torch/model.py:907] [7] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 04:43:39.403: W smdistributed/modelparallel/torch/model.py:917] [7] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 04:43:39.411: I smdistributed/modelparallel/torch/model.py:907] [8] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 04:43:39.411: W smdistributed/modelparallel/torch/model.py:917] [8] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 04:43:39.419: I smdistributed/modelparallel/torch/model.py:907] [10] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 04:43:39.420: W smdistributed/modelparallel/torch/model.py:917] [10] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 04:43:39.455: I smdistributed/modelparallel/torch/model.py:907] [0] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 04:43:39.455: W smdistributed/modelparallel/torch/model.py:917] [0] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 04:43:39.648: I smdistributed/modelparallel/torch/model.py:907] [1] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 04:43:39.648: W smdistributed/modelparallel/torch/model.py:917] [1] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/user_content.pt s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/user_content.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/special_tokens_map.json s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/tokenizer_config.json s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/config.json s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/added_tokens.json s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/added_tokens.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/training_args.bin s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/training_args.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/generation_config.json s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/generation_config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/pytorch_model.bin.index.json s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/tokenizer.model s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/tokenizer.model\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/pytorch_model-00002-of-00002.bin s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/pytorch_model-00002-of-00002.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/pytorch_model-00001-of-00002.bin s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/pytorch_model-00001-of-00002.bin\u001b[0m\n",
      "\u001b[34m2023-05-21 04:44:41,082 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-21 04:44:41,082 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-21 04:44:41,082 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-05-21 04:45:16 Uploading - Uploading generated training model\u001b[35m2023-05-21 04:44:41,094 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[35m2023-05-21 04:45:11,125 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2023-05-21 04:45:11,125 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-05-21 04:45:22 Completed - Training job completed\n",
      "Training seconds: 8520\n",
      "Billable seconds: 8520\n"
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "train_input_path = 's3://your_bucket/samples/datasets/1536-token-length-for-llama/train'\n",
    "test_input_path = 's3://your_bucket/samples/datasets/1536-token-length-for-llama/test'\n",
    "data = {\n",
    "    'train': train_input_path,\n",
    "    'test': test_input_path\n",
    "}\n",
    "\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.m5.large",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
